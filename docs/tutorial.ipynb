{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51f73001",
   "metadata": {},
   "source": [
    "# Tutorial: depoly the MNIST fashion model \n",
    "\n",
    "In this tutorial we will deploy the MNIST fashion model build in TensorFlow. Once the model is deployed, we will use the Python API to load data, prepare them and perform the inferences. \n",
    "\n",
    "\n",
    "First of all, we create a folder named \"mnist\" that contains our model, as described in the [model repository](http://localhost:8888/notebooks/docs/model_repository.md). \n",
    "\n",
    "![pre_post_processing.png](img/pre_post_processing.png)\n",
    "\n",
    "Inside the folder **1**, we put the model we want to deploy and all the needed files for that model.\n",
    "\n",
    "The **config.pbtxt** contains the model configuration that we want to use for this model. Please refer to the [model configuration](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md) documentation\n",
    "\n",
    "![mnist_config.png](img/mnist_config.png)\n",
    "\n",
    "Second, we create a file in the same folder named \"mnist.py\", where we will define two methods: preprocessing and postprocessing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0c0fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist.py \n",
    "\n",
    "import numpy as np \n",
    "\n",
    "def preprocessing(data): \n",
    "\treturn data \n",
    "\n",
    "def postprocessing(predictions): \n",
    "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    \n",
    "    res = [class_names[np.argmax(pred)] for pred in predictions]\n",
    "    return res \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab580e4c",
   "metadata": {},
   "source": [
    "Well, the model is ready to be served. \n",
    "\n",
    "Now, we need to setup the Triton Inference Server and run it using the following command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb2800c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: path_to_project/scripts/env_stack.sh: Aucun fichier ou dossier de ce type\n",
      "/bin/bash: triton : commande introuvable\n"
     ]
    }
   ],
   "source": [
    "! source path_to_project/scripts/env_stack.sh\n",
    "! triton start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d9813",
   "metadata": {},
   "source": [
    "Let's verify that the server is running: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e2d04e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: triton : commande introuvable\r\n"
     ]
    }
   ],
   "source": [
    "! triton is_alive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef7cab",
   "metadata": {},
   "source": [
    "As the Triton Inference Server is now running and the model is deployed , we can start performing inferences.\n",
    "\n",
    "Let's create a file that you name as you like. We chose \"test_mnist.py\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329508bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.getenv(\"TRITON\"))\n",
    "\n",
    "# we import the python API\n",
    "from src.triton import utils\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # We start by get the FLAGS passed as arguments\n",
    "\tFLAGS = utils.get_flags()\n",
    "    \n",
    "    #We create our inference engine based on the received flags  \n",
    "\tinfer_engine = utils.setup(FLAGS)\n",
    "\n",
    "    #We load fashion_mnist dataset from keras\n",
    "    \n",
    "\tfashion_mnist = keras.datasets.fashion_mnist\n",
    "\t(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\ttest_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "\t#We prepare requests tthat will be sent to the server \n",
    "    requests = utils.prepare_requests(FLAGS, test_images)\n",
    "    \n",
    "    #We sned asynchronously requests to the server\n",
    "\tpredictions = utils.async_infer(infer_engine, requests)\n",
    "\n",
    "\tfor i in range(len(predictions)):\n",
    "\t\tprint( \"predicted: \",  predictions[i] ,\". Actual : \" , test_labels[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a7e314b",
   "metadata": {},
   "source": [
    "![results.png](img/results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023a76af",
   "metadata": {},
   "source": [
    "That's it. Now you are ready to deploy your own model! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
